\documentclass[paper.tex]{subfiles}
\begin{document}

\section{Introduction}
\label{sec:introduction}

Floating point inaccuracy is notoriously difficult to detect and
debug~\cite{berkeley00-needle-like, kahan-java-hurts}.  Rounding
errors have lead to irreproducibility and even retraction of
scientific articles~\cite{num-issues-in-stat, num-replication,
  stat-robustness}, legal regulations in finance~\cite{euro-rounding},
distorted stock market indices~\cite{distort-stock,
  wall-street-distort-stock}, and skewed election
results~\cite{round-elections}.  Many applications that must produce
accurate results, including physical simulators and statistical
packages, depend on floating point hardware to efficiently approximate
computations over real numbers.  While such approximations make these
computations feasible, they also introduce rounding error, which can
lead to unacceptable \textit{accuracy}, that is, the approximate
results may differ from the ideal real number results by an
unacceptable margin.

% The efficiency provided by hardware implemented floating point
% enables these applications to effectively process large datasets.

In practice, developers often respond to rounding error by increasing
\textit{precision}, the size of the floating point representation.
For example, a developer might try to shift error to lower order bits
by replacing all 32-bit, single precision floats with 64-bit, double
precision floats.  Unfortunately, even the largest hardware-supported
precision may still exhibit unacceptable rounding error, and further
increases to precision require simulating arbitrary precision floating
point in software, incurring orders of magnitude
slowdown\footnote{Even arbitrary precision floating point can exhibit
  rounding error, and so the developer must still carefully select a
  precision that provides sufficient accuracy.}.

The numerical methods literature provides techniques to analyze and
mitigate rounding error without increasing precision.  Forward and
backward error analysis \todo{cite} can be applied to measure the
error of a program, and various program transformations \todo{cite}
can be applied to improve accuracy.  Numerical methods is a vast
field, which can also support programmers in the accuracy of matrix
computations, avoid accumulating error in a loop, and replacing
unstable algorithms with numerically stable ones.  Unfortunately,
these techniques often require both understanding the subtle details
of floating point arithmetic and also manually rearranging
computations.

As a first step toward addressing these challenges, we introduce
\casio, a technique for automatically mitigating rounding error in
floating point computations.  \casio searches for error-reducing
program transformations by combining three novel techniques.  First,
\casio determines which operations are responsible for rounding error
by sampling floating point inputs and comparing the results of
intermediate operations against accurate results computed using
arbitrary precision floating point.  Second, \casio applies a database
of rewrite rules to the error's source to produce program variants
that are evaluated to determine which variants have higher accuracy
than the original program. Third, \casio detects when two or more
program variants have complementary error behavior on different parts
of the input space and combines these programs to improve overall
accuracy. Thus \casio improves accuracy automatically, without
requiring the programmer to learn the details of floating point
arithmetic or manually rearrange computations.

We evaluate \casio on examples drawn from a classic numerical methods
textbook and consider its broader applicability to floating point
expressions extracted from a mathematical library as well as formulas
from recent scientific articles.  Our results demonstrate that \casio
can effectively discover transformations that substantially improve
accuracy (\todo{quantitative result here}) while imposing much lower
overhead than software floating point (\todo{quantitative result
  here}).  Furthermore, \casio has already been applied by colleagues
in machine learning who were able to significantly reduce rounding
error that arose when computing probabilities in a clustering
algorithm.

\medskip

\noindent The contributions of this paper include:
\begin{itemize}
\item The first automated approach to improving floating accuracy by
  rearranging computations;
\item A definition of \textit{real-equivalence} which provides
  correctness criteria for accuracy-improving transformations;
\item A heuristic search for accuracy-improving program
  transformations whose sampling-based approach relies on
  real-equivalence to avoid overfitting.
\end{itemize}

% In summary, \casio contributes three novel techniques to automatically
% improve floating point accuracy:
% \begin{itemize}
% \item A goal directed search for accuracy-improving program
%   transformations, guided by techniques that estimate rounding error
%   and its sources;
% \item A technique that iteratively applies local, error-specific
%   rewrites, causing traditional error-reducing program transformations
%   to fall out naturally;
% \item Approaches to both detecting regimes where error behavior
%   changes and also combining multiple program variants to reduce
%   overall error.
% \end{itemize}

\medskip

\noindent The rest of the paper is organized as follows.  \Cref{sec:background}
provides a brief background on floating point arithmetic.
\Cref{sec:overview} illustrates \casio on a representative example and
describes the high level \casio workflow.  \Cref{sec:synthesis}
details \casio's heuristic search and error estimation techniques.
\Cref{sec:evaluation} evaluates \casio's effectiveness at improving
accuracy and measures \casio's performance overhead.
\Cref{sec:relatedwork} surveys the most closely related work while
\Cref{sec:futurework} considers future directions for \casio.


% GRAVEYARD

% Unfortunately, while most programmers are instilled with a sort of
% superstitious unease concerning floating point, very few possess the
% background required to manually apply the traditional numerical
% methods techniques that address these challenges.

% Many developers are instilled with a sort of superstitious unease
% concerning floating point.

% not only make the hard won knowledge of the numerical methods
% community more generally accessible to non-experts, but also help
% make experts more effective

% \casio complements several recent efforts to improve overall floating
% point computation.  Our technique is parameterized by error estimation
% so we can adopt state of the art techniques as they are developed.
% STOKE can work as a post processing pass.

% In particular, researchers have developed techniques to
% automatically measure error, automatically choose between available
% hardware-supported precisions, and optimize computations to avoid
% expensive operations for precision that is not required.

% However, this work is often carried out under the assumption that
% the original floating program is the ``ground truth'', that is, that
% it is already an acceptable approximation of the original real
% computation the programmer had in mind.  Unfortunately,
% approximating real computation in floating point presents many
% subtle challenges which, if not accounted for, can yield arbitrarily
% large errors.  This limits the value of recent advances improving
% the runtime and power efficiency of such computations: the wrong
% answer is simply arrived at more quickly using fewer joules (watts?
% ugh).

% Casio works by performing a heuristic search over the space of
% programs equivalent to the input program over the reals.  Each step
% of the search attempts to apply identity transformations over the
% reals that have been shown valuable in the numerical methods
% literature.  Casio also searches for alternate representations of
% programs that may be challenging for even for numerical methods
% experts to develop, for example, special casing variants of the
% program to input domains with better precision for that variant.

\end{document}
