\documentclass[paper.tex]{subfiles}
\begin{document}

\section{Introduction}
\label{sec:introduction}

From scientific computing to financial forecasting, many applications
depend on hardware-supported floating point arithmetic. Developers of
these applications typically use floating point to efficiently
approximate computation over real numbers.  Unfortunately, such finite
approximations introduce rounding error.  For many computations, even
miniscule errors can compound to produce completely incorrect results.

The consequences of rounding error can be dire. \todo{examples where
  this is real bad -- retracted scientific articles}

In practice, developers have few tools to address rounding error.  A
common strategy is simply increasing precision, for example by
replacing 32-bit floats with 64-bit floats.  While increasing
precision may help by shifting error to lower order bits, the program
can still exhibit unacceptable rounding error.  Furthermore increasing
precision typically doubles floating point space and time costs. If
the largest hardware-supported precision cannot produce sufficiently
accurate results, developers may be forced to incur the high costs of
simulating arbitrary precision floating point in software.  In our
experiments, one such library imposed \todo{mpfr overhead} runtime
overhead to achieve \todo{NNN} bits of accuracy which required
\todo{NNN} bits of precision on average. Even this approach is not a
panacea, as the developer must still determine the precision
sufficient to provide the desired accuracy, a task that requires
expertise and manual effort.

The numerical methods literature provides numerous techniques to
analyze and mitigate rounding error, including sophisticated forward
and backward error analysis \todo{cite}, robust floating point
representations offering multiple precisions and designed for
efficient hardware implementation \todo{cite}, and numerous program
transformations to reduce error \todo{cite} without increasing
precision.  Unfortunately, applying these techniques requires deep
expertise and substantial manual effort.  Numerical methods techniques
rely on a deep understanding of the floating point representation,
sophisticated error analyses to choose an appropriate precision, and
program transformations to improve accuracy when additional precision
is unavailable or too expensive.

To address these challenges, we introduce \casio, an
\textit{automated} technique to mitigate rounding error without
resorting to software floating point.  Given a real number formula,
\casio combines three novel techniques to search for a floating point
implementation that improves accuracy: (1) an approach to estimating
error and its causes which is tailored to managing the high branching
factor of \casio's search; (2) an approach to detecting regimes where
error behavior changes which allows \casio to combine multiple
programs and reduce overall error; (3) a technique that iteratively
applies local, domain specific rewrites, from which traditional
error-reducing program transformations fall out naturally.  At each
step of the search, \casio automatically analyzes a candidate program
for sources of error, applies potentially error-reducing rewrites
inspired by the numerical methods literature, and estimates the
rounding error of the resulting program.  By searching through
thousands of candidate programs, our prototype implementation is able
to \todo{impressive summary of evaluation}.  Furthermore, \casio
reduces rounding error automatically, requiring no training or manual
intervention from the programmer.

We evaluated \casio on a large suite of microbenchmarks drawn from
numerical methods textbooks, mathematical libraries drawn from the
Web, and recent scientific articles.  Our results show that \casio
\todo{results}.  We also evaluated \casio on two large applications
\todo{do it}.  Finally, \casio has already been found useful by
colleagues in machine learning who were able to significantly reduce
the rounding error of their learning algorithm something something.
We evaluate \casio on benchmarks drawn from numerical methods
textbooks, mathematical libraries, and recent scientific articles,
demonstrating that \casio substantially reduces error while imposing
modest overhead.

In summary, this paper presents \casio, an \textit{automated}
technique to improve accuracy by applying transformations inspired by
the numerical methods literature. To achieve this goal required three
primary contributions:
\begin{enumerate}
\item A goal directed search for floating point accuracy, guided by
  techniques that estimate rounding error and its causes,
  and heuristics to manage the high branching factor of this search
\item An approach to detecting regimes where error behavior changes,
  and techniques that combine multiple versions of a floating point
  program to best reduce error over all regimes
\item A technique that iteratively applies local, domain specific rewrites,
  causing traditional error-reducing program transformations
  to fall our naturally
\end{enumerate}
Furthermore, we evaluate our technique on \todo{microbenchmarks and
  application} and demonstrate that we can substantially improve
accuracy while imposing only modest overhead.  Our implementation also
serves as a foundation for other to build upon.

The rest of the paper is organized as follows.  \Cref{sec:background}
provides a brief survey of the relevant background on floating
computation.  \Cref{sec:overview} illustrates \casio on a
representative example and describes the high level \casio workflow.
\Cref{sec:synthesis} details \casio's heuristic search and error
estimation techniques.  \Cref{sec:evaluation} evaluates \casio's
effectiveness across a suite of microbenchmarks, two large
applications, and the effort to build and run \casio.
\Cref{sec:relatedwork} surveys the most closely related work and
\Cref{sec:futurework} considers future work and concludes.


% GRAVEYARD

% Unfortunately, while most programmers are instilled with a sort of
% superstitious unease concerning floating point, very few possess the
% background required to manually apply the traditional numerical
% methods techniques that address these challenges.

% Many developers are instilled with a sort of superstitious unease
% concerning floating point.

% Furthermore, such incorrect results are difficult to detect and
% debug.

% not only make the hard won knowledge of the numerical methods
% community more generally accessible to non-experts, but also help
% make experts more effective

% ideal real number computation

% Applying numerical methods techniques requires substantial expertise
% and manual effort: analysis + transformation.  As more and more
% scientists write numerical programs, the lack of accuracy becomes
% increasingly troubling.  These non-expert programmers usually lack
% the training and time necessary to employ numerical methods in their
% code, and often may not even be aware of the subtle challenges this
% domain presents.

% \casio complements several recent efforts to improve overall floating
% point computation.  Our technique is parameterized by error estimation
% so we can adopt state of the art techniques as they are developed.
% STOKE can work as a post processing pass.

% ---


% In particular, researchers have developed techniques to
% automatically measure error, automatically choose between available
% hardware-supported precisions, and optimize computations to avoid
% expensive operations for precision that is not required.


% In practice, programmers often attempt to avoid numerical inaccuracy
% by simply increasing precision, for example by replacing 32-bit floats
% with 64-bit floats.  However, choosing the right representation is
% difficult.  Speed and space and precision tradeoffs.  Choosing right
% tradeoff requires error analysis.  Several techniques exist to
% automatically analyze floating point precision errors CITE CITE CITE
% and help the developer choose an appropriate precision CITE CITE CITE
% \todo{Precimonious.  Lam from UMD.  lots of other automated error
%   analysis}.  Note that, even when applications can afford to simulate
% arbitrary precision floats in software, these challenges still arise.

% Hardware-supported floating point representations come in coarse,
% exponentially growing sizes.  Recent work from the approximate
% computing community helps avoid paying speed and energy overheads for
% precision the developer may not need. \todo{STOKE. Luis. Martin.}  New
% verification techniques help the programmer ensure they have all the
% bits they think they need \todo{eva}.  Testing for overflow,
% underflow, error \todo{popl13}.

% However, to the best of our knowledge, one traditional task of the
% numerical methods expert remains manual: rewriting the floating
% program to \textit{improve} accuracy.



% However, this work is often carried out under the assumption that
% the original floating program is the ``ground truth'', that is, that
% it is already an acceptable approximation of the original real
% computation the programmer had in mind.  Unfortunately,
% approximating real computation in floating point presents many
% subtle challenges which, if not accounted for, can yield arbitrarily
% large errors.  This limits the value of recent advances improving
% the runtime and power efficiency of such computations: the wrong
% answer is simply arrived at more quickly using fewer joules (watts?
% ugh).

% The field of numerical methods provides a rich set of techniques for
% improving the numerical accuracy of real computations approximated
% in floating point.  Unfortunately, applying these techniques has
% traditionally demanded extensive training and expensive manual
% analysis and optimization of numerical programs.  As more and more
% scientists write numerical programs, this lack of accuracy becomes
% increasingly troubling.  These non-expert programmers usually lack
% the training and time necessary to employ numerical methods in their
% code, and often may not even be aware of the subtle challenges this
% domain presents.

% We present a new tool, \casio, which synthesizes floating point programs
% which better approximate the real computations programmers have in mind.
% Casio works by performing a heuristic search over the space of programs
% equivalent to the input program over the reals.  Each step of the search
% attempts to apply identity transformations over the reals that have been
% shown valuable in the numerical methods literature.  Casio also searches
% for alternate representations of programs that may be challenging for even
% for numerical methods experts to develop, for example, special casing
% variants of the program to input domains with better precision for that
% variant.

\end{document}