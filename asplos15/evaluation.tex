\documentclass[paper.tex]{subfiles}
\begin{document}

\section{Evaluation}
\label{sec:evaluation}

We evaluate \casio on benchmarks drawn from Hamming's \emph{Numerical
  Methods for Scientists and Engineers}~\cite{book87-nmse}, a standard
numerical analysis textbook.  Our evaluation includes all twenty-eight
worked examples and problems from Chapter~3, which discusses manually
rearranging formulas to improve accuracy, the same task that \casio
automates.

All experiments were carried out on an Intel Core~i5-3317U with 4GB
RAM running Fedora Linux 19, PLT Racket 5.3.6, and GCC 4.9.1.  When
compiling programs to C for performance measurements, we used GCC
flags \texttt{-march=native}, \texttt{-mtune=native},
\texttt{-mfpmath=both}, \texttt{-O3}, and \texttt{-flto}.

Overall, our results demonstrate that \casio can improve accuracy for
21 out of our 29 benchmark, often
without significantly slowing execution (average 45\% overhead).
In all cases, the programs produced by \casio run orders of
magnitude faster than software floating point
(a slowdown of two or more orders of magnitude).  In some cases, the accuracy of the programs output
by \casio even exceeds that solutions provided by experts in numerical
methods textbooks.  We also separately evaluate our error estimation
technique and our regime inference algorithm, and describe several
case studies using \casio.

\begin{figure}
  \centering
    \includegraphics[width=0.9\columnwidth]{fig/eval-rect-d.pdf}
    \caption{Accuracy Results. Each row of the figure represents a
      single benchmark. For each benchmark, the accuracy of the input
      program is drawn as the left edge of a rectangle, and the
      accuracy of \casio's output is drawn as the right edge.  The
      benchmarks are sorted from top to bottom in increasing order of
      the accuracy of \casio's output. The horizontal axis measures
      accuracy in terms of the number of bits of output that agree
      with the exact result, averaged across one million input
      points. Target programs were evaluated using double precision
      64-bit floats. For those inputs which \casio is not able to
      improve by at least half a bit, a dot is drawn at the input
      accuracy level. }
  \label{fig:eval-rect}
\end{figure}

\para{Accuracy} Our results for are shown in
Figure~\ref{fig:eval-rect}.
\casio improves the program's accuracy for 21 out of 28 programs;
for 20 programs, this is an improvement of at least one bit.
Of the seven benchmarks that \casio does not improve,
  NMSE suggests solutions which are not real-equivalent to the input
  because they use Taylor expansions to recover precision near 0. While \casio cannot
currently improve these benchmarks, adding support for unsound
rewrites such as Taylor expansions is an interesting direction for
future work.

\begin{figure}
  \centering
  \includegraphics[width=0.9\columnwidth]{fig/eval-overhead-d.pdf}
  \caption{Cumulative distribution of overhead induced by \casio. The
    horizontal axis shows the ratio between the performance of the
    output and input programs. Programs run using double precision
    floats and compute on one million randomly chosen points.}
  \label{fig:eval-overhead}
\end{figure}

\para{Overhead}
We also the performance of both the original program and the program
produced by \casio. On average, the program produced by \casio is 45\% slower,
much smaller than a slowdown of two orders of magnitude for using
software arbitrary precision.

\begin{figure}
\includegraphics[width=0.9\columnwidth]{fig/eval-casio-time.pdf}
\caption{Cumulative distribution of running time of the \casio tool on
  each benchmark. The horizontal axis shows time in seconds. Most
  benchmarks complete in less than two minutes.}
\label{fig:eval-casio-time}
\end{figure}

\para{Runtime}
\casio was run on each benchmark in a standard configuration%
\footnote{Including regime inference and the normal number of sample
  points.}.  The main loop was capped at \nIters iterations.
Figure~\ref{fig:eval-casio-time} is a cumulative distribution function for
\casio's runtime on these benchmarks.  For 15 of 29 tests, \casio runs
in under one minute.

\begin{figure}
\includegraphics[width=0.9\columnwidth]{fig/eval-mpfr-bits.pdf}
\caption{Cumulative distribution of precision required to evaluate
  each benchmark to 64 bits of accuracy. The horizontal axis shows
  bits of precision. Results were obtained by iteratively increasing
  the precision until the high-order 64 bits converged.}
\label{fig:eval-mpfr-bits}
\end{figure}

\para{Error Estimation}
The accuracy and speed was measured, both for the original program in
each benchmark, and for \casio's output.  Examples were compiled to C,
which was then compiled by the GNU Compiler Collection with flags
optimizing for performance without loss of accuracy%
\footnote{Version $4.9.1$ of \texttt{gcc} was used, with flags
  \texttt{-march=native}, \texttt{-mtune=native},
  \texttt{-mfpmath=both}, \texttt{-O3}, and \texttt{-flto}.
  Experiments were run on an Intel Core~i5-3317U processor.}.
Both the input and output
program were run with double precision intermediates.  Each program was run on the same set of
1,000,000 points drawn randomly from the set of single-precision
floating point numbers.  Results were compared with a ground truth
computed via the MPFR arbitrary-precision library~\cite{acm07-mpfr}.
Figure~\ref{fig:eval-mpfr-bits} shows the cumulative distribution of
the number of bits necessary to exactly evaluate the benchmarks.  The
error of an output computed in floating-point, was taken to be the
base-2 logarithm of the number of 64-bit floating point values%
\footnote{Inputs are single precision values, while outputs are double
  precision.  Single precision for inputs avoids handicapping
  implementations that use single precision for intermediate values.
  Double precision for outputs allows measuring error more accurately
  than single precision would.}, between the exact and inexact answer:
the bits in error for the approximate output, compared to the exact
answer.  This error was averaged over all points for which the exact
answer was a finite floating point value, resulting in a simple number
which can be used for comparing implementations.  Sampling one million
points is a very accurate measure of the average error, as
demonstrated in Section~\ref{sec:eval-eval}.

\subsection{Error Evaluation}
\label{sec:eval-eval}

\begin{figure}
\includegraphics[width=0.9\columnwidth]{fig/eval-err.pdf}
\caption{Standard error vs. sample size. Each point represents one
  hundred runs of \casio on the quadratic formula benchmark with
  varying sample size used to estimate error of program variants. The
  horizontal axis shows the sample size. The vertical axis shows
  standard error of the mean of the error estimates produced by the
  one hundred runs. By default, \casio runs the search with a sample
  size of 1024, which has a standard error of approximately 0.1.}

\label{fig:eval-err}
\end{figure}

The tests above evaluated \casio by comparing the average error of its
input and output.  Average error was computed by sampling one million
points and computing the average error across the sample.  To
demonstrate that this accurately estimates average error, we computed
average error with the number of points $N$ varying between 1 and
10,000.  For each $N$, the experiment was repeated 100 times with
different sampled points.  The standard error of the mean of these 100
repetitions, for each value of $N$, measures the accuracy of our
computation of average error.  Figure~\ref{fig:eval-err} is a
representative result; it was generated from the the quadratic formula
example described in Section~\ref{sec:overview}, but every test case
generated a qualitatively similar result.  In each case, sampling a
thousand points gave a result with standard error of approximately
$0.1$ bits.  This demonstrates that the numbers given above are
accurate.

To ensure that our evaluation in arbitrary precision is accurate (that
is, to ensure that we use a sufficiently large number of bits to
evaluate the results accurately), we computed the exact answer for
both \casio's input and output in arbitrary precision.  In every case,
the answers were identical, showing that arbitrary precision
evaluation is accurate.

\subsection{Regime Inference} \label{sec:eval-regimes}

\begin{figure}
\includegraphics[width=0.9\columnwidth]{fig/eval-regimes-e2e.pdf}
\caption{Evaluation of regimes inference. For each of the four
  programs for which regimes improve accuracy significantly, a
  rectangle is drawn. The left edge of the rectangle is the unimproved
  program. The middle line, if any, is \casio's output without
  regimes. The right line is \casio's output with regimes. If no
  middle line is visible, \casio without regimes was not able to
  improve the program.}
\label{fig:eval-regimes-e2e}
\end{figure}


Two experiments evaluate \casio's regime inference.  The first is an
end-to-end test, running \casio on its standard benchmarks but with
regime inference turned off.
Comparing the results of \casio with and
without regime inference measures the effectiveness of regime
inference.
The same sample points were used to make the results directly comparable.
Regime inference adds branches to four of the 29
benchmarks; Figure~\ref{fig:eval-regimes-e2e} shows \casio's accuracy
and speed on these four benchmarks, with and without regime
inference.  The results show that for these test cases, regime
inference makes a significant improvements to the program's accuracy.
However, the added branches also make the resulting programs slower.
Regime inference infers branches of the form $a_< < x_i < a_>$, with
$x_i$ a program variable.
Since points are selected randomly, these branches are
impossible for the processor to predict;
in practical use, where points are not randomly chosen,
the overhead may be lower.
Users may opt to disable regime inference if this performance cost is unacceptable.

A second experiment measured regime inference in isolation.
An input point $p$ is chosen and regimes is asked to combine the simple
programs $sqrt(x)$, $sin(x)$, $cos(x)$, $log(x)$, $sqr(x)$,
and $abs(x) + 10$, to most accurately compute $(\mathsf{if}\:x <
p\:\mathsf{then}\:a\:\mathsf{else}\:b)$, for $a$ and $b$ chosen from
the same candidate programs.  Regime inference must infer a branch of the
form $x < p'$, with the correct candidate programs on each side of
this branch, to succeed.
The difference between the computed value $p'$ and
the ground truth $p$ measures regime inference's ability to accurately
infer branch conditions.  This experiment was repeated 40 times.  In
each case, regime inference inferred a branch of the correct form; the
difference between $p$ and $p'$ varied between 0 and 1 bits of error.
This suggests that regime inference infers branch conditions very
accurately.

\subsection{Wider applicability}

To test \casio's applicability beyond its standard benchmark suite, we
gathered mathematical formulas from a variety of sources and tested
both their numerical accuracy and \casio's ability to improve it.
These sources included several papers from Volume 89 of Physical
Review; standard definitions of mathematical functions, such as
hyperbolic trigonometric functions, arithmetic on complex numbers; and
approximations to special functions like \textsf{erf} and $\zeta$.  Of the
68 formulas gathered, we found that 20 exhibited significant floating
point inaccuracies.  For these 20 examples, \casio was able to improve
6 with no modifications and without enlarging its database of rules.
This is yet another confirmation that rounding error can arise in the
daily practice of scientists and engineers.  However, it is important
to note that for these examples we have not determined if inaccuracies
arise for valid inputs; and, for formulas \casio was unable to
improve, whether a real-equivalent formula with lower error exists.

\subsection{Case Study: Computing Probabilities}

Many numerical programs are not library functions or textbooks
examples, but are instead simulations or data analysis scripts used by
scientists, engineers, and mathematicians in the course of their work.
The expressions encountered in these programs are more complex and
less structured than examples like~\eqref{eq:ex}.  \casio has
demonstrated success in improving precision in these more-difficult
cases.

A colleague of ours had difficulties with the numerical precision of a
MCMC update rule in a clustering algorithm.  He needed to compute the
expression
\begin{align*}
\mathsf{ans} &= \frac{p(e_-,e_+|s)}{p(e_-,e_+|t)} \\
p(e_-, e_+|x) &= (\operatorname{sig} x)^{e_+} (1 - \operatorname{sig} x)^{e_-} \\
\operatorname{sig}x &= 1 / (1 - e^{-x})
\end{align*}
and found that the simple encoding of this formula as a program lead
to spurious results and violated invariants in later code.  Our
estimates suggest that this simple encoding produces seventeen bits of
error, averaged over floating point values.  To avoid these problems,
our colleague manually manipulated the expression into a form that
seemed not to cause similar problems; our estimates suggest that this
improved variant had ten bits of average error.

Upon hearing of his troubles, we fed the original, naive
implementation to \casio.
\casio produced a program with four bits average error:
\begin{equation*}
  \operatorname{exp}\left(e_+\ln{\frac{1+e^{-t}}{1+e^{-s}}} +
     e_-\ln{\frac{1-\frac{1}{1+e^{-s}}}
                 {1-\frac{1}{1+e^{-t}}}}\right)
\end{equation*}
\casio obviated the need for manual algebraic manipulation, and
produced superior results with no manual steps.
The low-error program that \casio produced was so complex that human
analysts would be unlikely to reproduce the result.  Furthermore, our
attempts to simplify the program into a more understandable form have only been able to do so by greatly increasing
its error.  By properly targetting its search, \casio is able to
consider a greater number of viable alternatives than human analysts
are able to, and is capable of producing better results.

\end{document}
